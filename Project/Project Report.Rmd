---
title: "DATA 605 Final Project"
author: "Ilya Kats"
date: "December 20, 2017"
output:
  html_document:
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options("scipen" = 10)
```

```{r}
# Required libraries
library(MASS)
library(psych)
```

This project is based on data from the _House Prices_ competition on Kaggle (https://www.kaggle.com/c/house-prices-advanced-regression-techniques). Full data description is available [here](https://raw.githubusercontent.com/ilyakats/CUNY-DATA605/master/Project/data_description.txt).

### Data Import and Overview

```{r}
# Import training data
train <- read.csv('https://raw.githubusercontent.com/ilyakats/CUNY-DATA605/master/Project/train.csv')

# Get general size of the data set
dim(train)
```

Some variables, such as `LotArea`, should be correlated with the sale price. One variable that caught my eye as less obvious one is `LotFrontage`, linear feet of street connected to property.

```{r}
summary(train$LotFrontage)
```

There are 259 NAs out of 1,460 observations - still a decent amount of data to consider. The numbers seem like this may work for analysis. Let us assign our $X$ variable, `LotFrontage`, and our $Y$ variable, `SalePrice`.

```{r}
X <- train$LotFrontage
Y <- train$SalePrice
```

### Probability

My chosen variable, `LotFrontage`, has some NA values. For this section, I have decided to remove all observations with NA.

```{r}
probdata <- train[, c("LotFrontage", "SalePrice")]
probdata <- probdata[!is.na(probdata$LotFrontage),]

summary(probdata$LotFrontage)
summary(probdata$SalePrice)
```

```{r}
# First quartile of X variable
x <- quantile(probdata$LotFrontage)[2]
# Second quartile / median of Y variable
y <- median(probdata$SalePrice)
```

```{r}
t <- c(nrow(probdata[probdata$LotFrontage<x & probdata$SalePrice<y,]),
       nrow(probdata[probdata$LotFrontage<x & probdata$SalePrice==y,]),
       nrow(probdata[probdata$LotFrontage<x & probdata$SalePrice>y,]))
t <- rbind(t,c(nrow(probdata[probdata$LotFrontage==x & probdata$SalePrice<y,]),
               nrow(probdata[probdata$LotFrontage==x & probdata$SalePrice==y,]),
               nrow(probdata[probdata$LotFrontage==x & probdata$SalePrice>y,])))
t <- rbind(t,c(nrow(probdata[probdata$LotFrontage>x & probdata$SalePrice<y,]),
               nrow(probdata[probdata$LotFrontage>x & probdata$SalePrice==y,]),
               nrow(probdata[probdata$LotFrontage>x & probdata$SalePrice>y,])))
t <- cbind(t, t[,1] + t[,2] + t[,3])
t <- rbind(t, t[1,] + t[2,] + t[3,])
colnames(t) <- c("Y<y", "Y=y", "Y>y", "Total")
rownames(t) <- c("X<x", "X=x", "X>x", "Total")
knitr::kable(t)
```

a. $P(X>x | Y>y)=\frac{489}{598} \approx 0.8177$
b. $P(X>x\ and\ Y>y) = \frac{489}{1201} \approx 0.4072$
c. $P(X<x | Y>y)=\frac{101}{598} \approx 0.1689$

Additional probabilities can be calculated using the following probabilities table.

```{r}
knitr::kable(round(t/1201,4))
```

Consider probability (a) above: $P(X>x | Y>y)=0.8177$. $P(X>x)=0.7469$. 

Since $P(X>x | Y>y) \ne P(X>x)$, these events are **not independent**. 

#### Chi-squared Test

Let us the chi-squared test to evaluate the null hypothesis that $X>x$ (lot frontage is greater than first quartile or 59) and $Y>y$ (sale price is greater than median or 159,500) are independent events. Because we have very few events such that $X=x$ or $Y=y$, we will combine those values and only use two categories - $\le$ and $>$.

```{r}
chisq.test(table(probdata$LotFrontage>x, probdata$SalePrice>y))
```

The p-value is nearly zero. Therefore, we reject the null hypothesis. Two events are not independent.

### Descriptive and Inferential Statistics

Let us get some basic statistics about the `LotFrontage` variable.

```{r}
summary(X)
describe(X)
```

There are 1,201 valid observations between a very small/narrow lot of 21 feet and large lot of 313 feet. Average frontage is 70.05 feet.


Let us get some basic statistics about the `SalePrice` variable.

```{r}
summary(Y)
describe(Y)
```

Sale price is availavble for all 1,460 observations. It ranges from just shy of \$35,000 to just over \$750,000. Average sale price is $180,900. 

Let us evaluate a few plots. 

```{r}
par(mfrow=c(1,2))
boxplot(X, main="Boxplot of Lot Frontage")
hist(X, breaks=40, main="Histogram of Lot Frontage")
```

Looking at the `LotFrontage` boxplot and histogram, we can see that there are definitely outliers and distibution is right-skewed. 

```{r}
par(mfrow=c(1,2))
boxplot(Y, main="Boxplot of Sale Price")
hist(Y, breaks=40, main="Histogram of Sale Price")
```

Distribution of `SalePrice` is close to the distribution of `LotFrontage`. It is also right-skewed with some number of outliers. 

```{r}
plot(X, Y, xlab="Lot Frontage", ylab="Sale Price", 
     main="Scatterplot of Lot Frontage vs. Sale Price")
```

Looking at the scatter plot, there is no obvious correlation, but it is a bit hard to say definitely. Let us build and evaluate a linear regression model. 

```{r}
lm1 <- lm(Y ~ X)
summary(lm1)
```

Looking at the summary, we see that `LotFrontage` may carry some significance, but at the same time $R^2$ is very small and the model covers only about 12% of variability. More importantly looking at the analysis of residuals below it is clear that variability of residuals is not constant and distribution deviates from normal distribution. Because of these issues it would not be appropriate to use this model for analysis.

```{r}
plot(lm1$fitted.values, lm1$residuals, 
     xlab="Fitted Values", ylab="Residuals")
abline(h=0)
qqnorm(lm1$residuals); qqline(lm1$residuals)
```

#### Box Cox Transformation

Let us perform the Box Cox transformation to see if this model can be improved.

```{r}
bc <- boxcox(lm1)
(lambda <- bc$x[which.max(bc$y)])
```

It looks like the optimal $\lambda$ value is close to 0. In fact 0 is included in the 95% confidence interval. Let us try the $log$ transformation. 

```{r}
lm2 <- lm(log(Y) ~ X)
summary(lm2)
plot(lm2$fitted.values, lm2$residuals, 
     xlab="Fitted Values", ylab="Residuals")
abline(h=0)
qqnorm(lm2$residuals); qqline(lm2$residuals)

plot(X, log(Y), xlab="Lot Frontage", ylab="Log of Sale Price", 
     main="Scatterplot of Lot Frontage vs. Sale Price: Log Transform")
```

After _log_ transformaion of `SalePrice`, distribution of residuals is significantly closer to normal and there is noticeable improvement in variability. $R^2$ of the new model has not increased, but with Box Cox transformation the new model adheres closer to necessary assumptions. It is stil probably not enough, but there is improvement.

### Linear Algebra and Correlation

Let us select the following 4 variables from the data and build a correlation matrix:

- `TotalBsmtSF`: Total square feet of basement area
- `MoSold`: Month sold
- `OverallCond`: Rates the overall condition of the house (from _1-Very Poor_ to _10-Very Excellent_)
- `SalePrice`: Sale price

```{r}
cordata <- train[, c("TotalBsmtSF", "MoSold", "OverallCond", "SalePrice")]
cormatrix <- cor(cordata)
round(cormatrix,2)
```

Before analysis I suspected that month the house was sold in correlates to the sale price since sale decisions may be driven by school schedule and weather. This turned out to be not the case. Less surprisingly basement size does seem to correlate with the sale price (larger basement suggests larger house and higher sale price).

Let us invert the correlation matrix to get the precision matrix. 

```{r}
precmatrix <- solve(cormatrix)
round(precmatrix,2)
round(diag(precmatrix),2)
```

Variance inflation factors from the diagonal of the precision matrix indicate that `MoSold` and `OverallCond` are not correlated among 4 variables chosen for this analysis while `TotalBsmtSF` and `SalePrice` may have moderate correlation. 

Since $[Precision] = [Correlation]^{-1}$, then $[Precision]\times[Correlation]$ should be equal to **identity** matrix. Let us confirm.

```{r}
round(cormatrix %*% precmatrix,4)
round(precmatrix %*% cormatrix,4) == round(cormatrix %*% precmatrix,4)
```

### Calculus-Based Probability and Statistics

Let us compare the actual distribution of `LotFrontage` against gamma distribution using the `fitdistr` method of the `MASS` library.

```{r}
# Remove NAs
X <- X[!is.na(X)]

# Fitting of univariate distribution
(fd <- fitdistr(X, "gamma"))

# Actual vs simulated distribution
hist(X, breaks=40, prob=TRUE, xlab="Lot Frontage",
     main="Lot Frontage Distribution")
curve(dgamma(x, shape = fd$estimate['shape'], rate = fd$estimate['rate']), 
      col="blue", add=TRUE)
```

Looks like a very good fit. I picked the gamma distribution because the histogram of `LotFrontage` above seemed to resemble it. Now I am curious if perhaps another distribution would be a better fit based on `fitdistr`.

```{r warning=FALSE}
distributions <- c("cauchy", "exponential", "gamma", "geometric", "log-normal", "lognormal", 
                   "logistic", "negative binomial", "normal", "Poisson", "t", "weibull")
logliks <- c()
for (d in distributions) {
  logliks <- c(logliks, fitdistr(X, d)$loglik)
}

logtable <- as.data.frame(cbind(distributions, logliks))
knitr::kable(logtable[order(logtable$logliks),], 
             row.names=FALSE, col.names=c("Distribution", "Log-Likelihood"))
```

Based on log-loglikelihood values, it seems that the t distribution would be a better fit. 

```{r warning=FALSE}
(fd <- fitdistr(X, "t"))

# Actual vs simulated distribution
hist(X, breaks=50, prob=TRUE, xlab="Lot Frontage",
     main="Lot Frontage Distribution")
curve(dt(x, df = fd$estimate['df'], ncp=fd$estimate['m']), 
      col="blue", add=TRUE)
```

The graph is a bit inconclusive. I feel that I am missing something here in properly interpreting the t distribution, but I have decided to include the work anyway. 

### Modeling

#### Modeling Summary

The following are key steps taken in building the model:

1. Data was reviewed and number of variables were eliminated because they contained a lot of missing data or there was not enough variability in the data. 
2. Several variables were eliminated because they contained either repetetive data (represented by other variables) or data that is more likely to confuse the model rather than improve it. 
3. Categorical variables were converted to numerica values. 
4. Target variable, `SalePrice`, was log-transformed to bring it to the scale of other variables. 
5. Training and testing data sets were combined for data clean-up, but then separated for modeling.

Final model formula is as follows:

> formula = SalePrice ~ MSZoning + LotFrontage + LotArea + BldgType + 
    OverallQual + OverallCond + YearBuilt + RoofMatl + Exterior1st + 
    Exterior2nd + ExterCond + BsmtCond + BsmtFinType1 + BsmtFinSF1 + 
    HeatingQC + CentralAir + X1stFlrSF + GrLivArea + FullBath + 
    KitchenQual + Functional + GarageFinish + GarageArea + GarageCond + 
    PavedDrive + WoodDeckSF + SaleCondition

Kaggle username is **IlyaKats**. Final score is **0.13513**. Submission data is available at https://github.com/ilyakats/CUNY-DATA605/tree/master/Project. 

#### Modeling Work

```{r}
# Read test data and add SalePrice column
test <- read.csv('https://raw.githubusercontent.com/ilyakats/CUNY-DATA605/master/Project/test.csv')
test <- cbind(test, SalePrice=rep(0,nrow(test)))

# Get training data and review summary statistics
md <- train
summary(md)

# Combine with testing data to do global replacements
md <- rbind(md, test)

# Eliminate features with limited or missing data
md <- subset(md, select=-c(Street, Alley, LandContour, Utilities, 
                           LandSlope, Condition2, MasVnrArea, Heating, 
                           BsmtFinSF2, X2ndFlrSF, LowQualFinSF, BsmtFullBath, 
                           BsmtHalfBath, HalfBath, PoolQC, PoolArea, MiscVal, 
                           MiscFeature, Fence, ScreenPorch, Fireplaces,
                           EnclosedPorch, MoSold, YrSold))
```

Based on summary statistics above the following fields were eliminated from modeling because of a lot of missing data - `Street`, `Alley`, `LandContour`, `Utilities`, `LandSlope`, `Condition2`, `MasVnrArea`, `Heating`, `BsmtFinSF2`, `X2ndFlrSF`, `LowQualFinSF`, `BsmtFullBath`, `BsmtHalfBath`, `HalfBath`, `PoolQC`, `PoolArea`, `MiscVal`, `MiscFeature`, `Fence`, `ScreenPorch`, `Fireplaces`, `EnclosedPorch`.  Additionally, `Id` was eliminated because it carries no relevant information. We have established above that `MoSold` does not correlate with sale price, so it was eliminated. Finally, `YrSold` was eliminated. Although, year may play a factor, data covers 2006 through 2010 - a relatively short period that is unlikely to contain significant patterns. 

We are left with the following columns.

```{r}
colnames(md)
```

```{r}
md <- subset(md, select=-c(LotShape, YearRemodAdd, BsmtExposure, 
                           BsmtFinType2, TotalBsmtSF, TotRmsAbvGrd, 
                           FireplaceQu, GarageYrBlt, GarageCars))
```

After reviewing data dictionary, the following fields were eliminated - `LotShape`, `YearRemodAdd` (contains construction year if no remodeling was done which may negatively interfere with the model), `BsmtExposure`, `BsmtFinType2`, `TotalBsmtSF` (included in other variables), `TotRmsAbvGrd`, `FireplaceQu`, `GarageYrBlt`, `GarageCars`.

Categorical variables were converted to numerical values. Remaining NAs were replaced with zeros.

```{r}
md$Neighborhood <- as.integer(factor(md$Neighborhood))
md$MSZoning <- as.integer(factor(md$MSZoning))
md$LotConfig <- as.integer(factor(md$LotConfig))
md$Condition1 <- as.integer(factor(md$Condition1))
md$BldgType <- as.integer(factor(md$BldgType))
md$HouseStyle <- as.integer(factor(md$HouseStyle))
md$RoofStyle <- as.integer(factor(md$RoofStyle))
md$RoofMatl <- as.integer(factor(md$RoofMatl))
md$Exterior1st <- as.integer(factor(md$Exterior1st))
md$Exterior2nd <- as.integer(factor(md$Exterior2nd))
md$MasVnrType <- as.integer(factor(md$MasVnrType))
md$ExterQual <- as.integer(factor(md$ExterQual))
md$ExterCond <- as.integer(factor(md$ExterCond))
md$BsmtQual <- as.integer(factor(md$BsmtQual))
md$BsmtCond <- as.integer(factor(md$BsmtCond))
md$Electrical <- as.integer(factor(md$Electrical))
md$KitchenQual <- as.integer(factor(md$KitchenQual))
md$Functional <- as.integer(factor(md$Functional))
md$GarageType <- as.integer(factor(md$GarageType))
md$GarageFinish <- as.integer(factor(md$GarageFinish))
md$GarageCond <- as.integer(factor(md$GarageCond))
md$BsmtFinType1 <- as.integer(factor(md$BsmtFinType1))
md$PavedDrive <- as.integer(factor(md$PavedDrive))
md$SaleType <- as.integer(factor(md$SaleType))
md$SaleCondition <- as.integer(factor(md$SaleCondition))
md$Foundation <- as.integer(factor(md$Foundation))
md$HeatingQC <- as.integer(factor(md$HeatingQC))
md$GarageQual <- as.integer(factor(md$GarageQual))

md[is.na(md)] <- 0
```

Separate data into training and testing sets. Log-transform sales price in the training set.

```{r}
test <- md[md$SalePrice==0,]
md <- md[md$SalePrice>0,]

md$SalePrice <- log(md$SalePrice)

# Remove ID column from training data
md <- subset(md, select=-c(Id))

# Build initial model with all fields
sale_lm <- lm(SalePrice ~ . , data=md)
summary(sale_lm)
```

Optimize the model using `stepAIC` method.

```{r}
step_lm <- stepAIC(sale_lm, trace=FALSE)
summary(step_lm)
```

$R^2$ is over 0.86 which seems like a good fit. Check residuals.

```{r}
plot(step_lm$fitted.values, step_lm$residuals, 
     xlab="Fitted Values", ylab="Residuals", main="Fitted Values vs. Residuals")
abline(h=0)

qqnorm(step_lm$residuals); qqline(step_lm$residuals)
```

Residuals definitely point to a few outliers. The model can probably be improved if outliers are removed from the data. Residuals are approximately normally distributed. 

```{r}
# Predict prices for test data
pred_saleprice <- predict(step_lm, test)
# Convert from log back to real world number
pred_saleprice <- sapply(pred_saleprice, exp)
# Prepare data frame for submission
kaggle <- data.frame(Id=test$Id, SalePrice=pred_saleprice)
write.csv(kaggle, file = "submission.csv", row.names=FALSE)
```

#### Kaggle Submission

Kaggle username is **IlyaKats**. Final score is **0.13513**.

![](https://raw.githubusercontent.com/ilyakats/CUNY-DATA605/master/Project/submission.png)
